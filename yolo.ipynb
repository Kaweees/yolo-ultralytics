{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmRVjaV7AKHj"
      },
      "source": [
        "# Ultralytics Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "def get_token(token_name: str) -> str:\n",
        "    \"\"\"Get a token from the environment variables\n",
        "\n",
        "    Args:\n",
        "        token_name (str): The name of the token to get\n",
        "\n",
        "    Returns:\n",
        "        str: The token\n",
        "    \"\"\"\n",
        "    token = os.environ.get(token_name)\n",
        "    if token is None:\n",
        "        raise ValueError(f\"{token_name} not found in environment variables\")\n",
        "    return token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z74MVVFIi81o"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N96SEuzHIAhv"
      },
      "outputs": [],
      "source": [
        "# Update as needed\n",
        "base_dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(base_dir):\n",
        "    raise FileNotFoundError(f\"Base directory {base_dir} does not exist\")\n",
        "\n",
        "# Define the path where Ultralytics will store the final model and training metrics\n",
        "output_dir = base_dir + \"/export\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    raise FileNotFoundError(f\"No such directory {output_dir}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GECQFwaWAtAL",
        "outputId": "3a48809d-bc58-4721-e06e-a2d502574ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.235 \ud83d\ude80 Python-3.12.11 torch-2.7.0+cpu CPU (ARMv8 Processor rev 1 (v8l))\n",
            "Setup complete \u2705 (6 CPUs, 7.4 GB RAM, 193.4/1831.3 GB disk)\n"
          ]
        }
      ],
      "source": [
        "import ultralytics\n",
        "from roboflow import Roboflow\n",
        "from ultralytics import YOLO\n",
        "\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCk8mq8XHv_D"
      },
      "source": [
        "## YOLO11 Model Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VjtmzGDjvR7"
      },
      "source": [
        "YOLO11 builds on the versatility of YOLOv8 and supports multiple **computer vision tasks** using different **prefixes** and **sizes**.\n",
        "\n",
        "**Model Variants and Tasks** </br>\n",
        "\n",
        "| **Model**      | **Filename Prefix**   | **Task**                     |\n",
        "|---------------|---------------------|------------------------------|\n",
        "| YOLO11       | `yolo11`             | Object Detection             |\n",
        "| YOLO11-seg   | `yolo11-seg`         | Instance Segmentation        |\n",
        "| YOLO11-pose  | `yolo11-pose`        | Pose Estimation (Keypoints)  |\n",
        "| YOLO11-obb   | `yolo11-obb`         | Oriented Object Detection    |\n",
        "| YOLO11-cls   | `yolo11-cls`         | Image Classification         |\n",
        "\n",
        "**Available Model Sizes** </br>\n",
        "\n",
        "Each model type comes in multiple sizes for different performance needs:\n",
        "\n",
        "- **n** (Nano) \u2192 Smallest, optimized for low-power devices  \n",
        "- **s** (Small) \u2192 Balanced for speed and accuracy  \n",
        "- **m** (Medium) \u2192 More accurate, moderate speed  \n",
        "- **l** (Large) \u2192 High accuracy, requires more computing power  \n",
        "- **x** (Extra Large) \u2192 Highest accuracy, most computationally expensive  \n",
        "\n",
        "**Example Usage** </br>\n",
        "\n",
        "To use a specific model:\n",
        "```python\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolo11s-seg.pt\")  # Load the small version of YOLO11 for instance segmentation\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCf2r1vdH6tf"
      },
      "source": [
        "## Download Dataset from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJNC1w3REadY",
        "outputId": "83dc0649-509e-4b94-bc9f-ba7b4080b836"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ROBOFLOW_WORKSPACE_ID not found in environment variables",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Update the values for WORKSPACE_ID, PROJECT_ID, API_KEY, and PROJECT_VERSION\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m WORKSPACE_ID \u001b[38;5;241m=\u001b[39m \u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mROBOFLOW_WORKSPACE_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m PROJECT_ID \u001b[38;5;241m=\u001b[39m get_token(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROBOFLOW_PROJECT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m get_token(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROBOFLOW_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mget_token\u001b[0;34m(token_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(token_name)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in environment variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
            "\u001b[0;31mValueError\u001b[0m: ROBOFLOW_WORKSPACE_ID not found in environment variables"
          ]
        }
      ],
      "source": [
        "# Update the values for WORKSPACE_ID, PROJECT_ID, API_KEY, and PROJECT_VERSION\n",
        "WORKSPACE_ID = get_token(\"ROBOFLOW_WORKSPACE_ID\")\n",
        "PROJECT_ID = get_token(\"ROBOFLOW_PROJECT_ID\")\n",
        "API_KEY = get_token(\"ROBOFLOW_API_KEY\")\n",
        "PROJECT_VERSION = get_token(\"ROBOFLOW_PROJECT_VERSION\")\n",
        "\n",
        "rf = Roboflow(api_key=API_KEY)\n",
        "project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n",
        "version = project.version(PROJECT_VERSION)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCOMtCvsIPNN"
      },
      "source": [
        "## Training the YOLO Model with Ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JGx7upbg2vc"
      },
      "source": [
        "### General Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7gDbuCUhxW2"
      },
      "source": [
        "\n",
        "**Epochs**\n",
        "\n",
        "The `epochs` parameter defines how many complete passes the model makes through the entire training dataset. Just like in real life, repeating the practice (processing the data) helps the model learn and improve. However, performance gains diminish after a certain point, and too many epochs can lead to overfitting.\n",
        "\n",
        "**Data Augmentation**\n",
        "\n",
        "Data augmentation artificially increases the diversity of your training data, helping the model generalize better. While services like Roboflow can apply augmentations when you prepare your dataset, the Ultralytics API (`model.train()`) also has built-in augmentation capabilities.\n",
        "\n",
        "You can find the full list of augmentations controllable via the Ultralytics training configuration here: [Ultralytics Configuration Docs](https://docs.ultralytics.com/usage/cfg/) (Check the augmentation section).\n",
        "\n",
        "**Our Augmentation Strategy & Cautions:**\n",
        "\n",
        "* **Mosaic:** We will utilize the `mosaic` augmentation (`mosaic=1.0` in parameters). It's a powerful technique that combines multiple images, improving detection of objects in various contexts and scales.\n",
        "* **`fliplr` (Flip Left-Right):** **Be very careful with this!** `fliplr` flips the image horizontally. If your task involves directionality (e.g., detecting *left* vs. *right* lane lines), this augmentation will flip the image *but not the labels*. This means your model will incorrectly learn from images where the visual right lane is labeled as left, and vice-versa. **For such tasks, set `fliplr=0.0` to disable it.**\n",
        "* **Combined Augmentations:** Be mindful if you've already applied augmentations via Roboflow (or another service). Applying heavy augmentations *both* during dataset preparation *and* during Ultralytics training might overlay excessively, potentially making images unrecognizable or confusing for the model. Adjust settings to avoid overly strong combined effects.\n",
        "\n",
        "| **Metric / Loss**  | **Explanation** |\n",
        "|-------------------|---------------|\n",
        "| **Box Loss (`box_loss`)** | Measures how well the model's predicted bounding boxes match the ground truth. Lower is better. |\n",
        "| **Classification Loss (`cls_loss`)** | Evaluates how accurately the model assigns the correct class to detected objects. Lower is better. |\n",
        "| **Distribution Focal Loss (`dfl_loss`)** | Helps refine bounding box locations by improving precision at the edges. Lower is better. |\n",
        "| **Precision (`P`)** | The proportion of predicted objects that are correct (True Positives / (True Positives + False Positives)). Higher means fewer false positives. |\n",
        "| **Recall (`R`)** | The proportion of actual objects that were detected (True Positives / (True Positives + False Negatives)). Higher means fewer false negatives. |\n",
        "| **mAP50 (`Mean Average Precision @ IoU 0.5`)** | Measures how well predicted boxes match ground truth at **Intersection over Union (IoU) \u2265 0.5**. Higher means better accuracy. |\n",
        "| **mAP50-95 (`Mean Average Precision @ IoU 0.5:0.95`)** | A stricter evaluation, averaging mAP over IoU thresholds from **0.5 to 0.95** (harder to score high). Higher means better performance. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZuRRI6_Qb3X"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9zrM4tUTtKE",
        "outputId": "587f65b1-3199-4f56-b051-f54c5be161f1"
      },
      "outputs": [],
      "source": [
        "gpu_count = torch.cuda.device_count()\n",
        "device_str = \",\".join(str(i) for i in range(gpu_count)) if gpu_count > 0 else \"cpu\"\n",
        "\n",
        "print(f\"\ud83e\udde0 Training on: {device_str} ({gpu_count} GPU(s) detected)\")\n",
        "\n",
        "# model = YOLO(\"yolo11n-seg.pt\")\n",
        "model = YOLO(\"yolo11s-seg.pt\")  # Load the nano version of YOLO11 for instance segmentation\n",
        "\n",
        "results = model.train(\n",
        "    data=f\"{dataset.location}/data.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=8 * max(gpu_count, 1),  # Scale batch size with GPU count\n",
        "    workers=2,\n",
        "    device=device_str,  # \u2705 Multi-GPU support via comma-separated string\n",
        "    fliplr=0.5,\n",
        "    mosaic=1.0,\n",
        "    degrees=10.0,\n",
        "    translate=0.1,\n",
        "    scale=0.5,\n",
        "    shear=2.0,\n",
        "    perspective=0.001,\n",
        "    project=output_dir + \"/runs/segment\",  # Set project to create the 'runs/segment' subfolders\n",
        "    name=\"train\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO4yRM4-iONj"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c0Oz3yriPxO"
      },
      "outputs": [],
      "source": [
        "# Load the trained YOLO model from the specified path\n",
        "model_path = base_dir + \"/export/runs/segment/train/weights/best.pt\"\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Run evaluation on the test split using the provided dataset configuration\n",
        "metrics = model.val(split=\"test\", data=f\"{dataset.location}/data.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VW_qZcFiqzc"
      },
      "source": [
        "### Export as ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Pcme7qitUk"
      },
      "outputs": [],
      "source": [
        "model.export(format=\"onnx\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
