{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmRVjaV7AKHj"
      },
      "source": [
        "# Ultralytics Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "def get_token(token_name: str) -> str:\n",
        "    \"\"\"Get a token from the environment variables\n",
        "\n",
        "    Args:\n",
        "        token_name (str): The name of the token to get\n",
        "\n",
        "    Returns:\n",
        "        str: The token\n",
        "    \"\"\"\n",
        "    token = os.environ.get(token_name)\n",
        "    if token is None:\n",
        "        raise ValueError(f\"{token_name} not found in environment variables\")\n",
        "    return token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z74MVVFIi81o"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyaemmu0IFX6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N96SEuzHIAhv"
      },
      "outputs": [],
      "source": [
        "# Update as needed\n",
        "base_dir = os.getcwd()\n",
        "\n",
        "if not os.path.exists(base_dir):\n",
        "  raise FileNotFoundError(f'Base directory {base_dir} does not exist')\n",
        "\n",
        "# Define the path where Ultralytics will store the final model and training metrics\n",
        "output_dir = base_dir + '/export'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    raise FileNotFoundError(f\"No such directory {output_dir}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GECQFwaWAtAL",
        "outputId": "3a48809d-bc58-4721-e06e-a2d502574ac0"
      },
      "outputs": [],
      "source": [
        "%pip install -q ultralytics roboflow\n",
        "import ultralytics\n",
        "from roboflow import Roboflow\n",
        "from ultralytics import YOLO\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCk8mq8XHv_D"
      },
      "source": [
        "## YOLO11 Model Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VjtmzGDjvR7"
      },
      "source": [
        "YOLO11 builds on the versatility of YOLOv8 and supports multiple **computer vision tasks** using different **prefixes** and **sizes**.\n",
        "\n",
        "**Model Variants and Tasks** </br>\n",
        "\n",
        "| **Model**      | **Filename Prefix**   | **Task**                     |\n",
        "|---------------|---------------------|------------------------------|\n",
        "| YOLO11       | `yolo11`             | Object Detection             |\n",
        "| YOLO11-seg   | `yolo11-seg`         | Instance Segmentation        |\n",
        "| YOLO11-pose  | `yolo11-pose`        | Pose Estimation (Keypoints)  |\n",
        "| YOLO11-obb   | `yolo11-obb`         | Oriented Object Detection    |\n",
        "| YOLO11-cls   | `yolo11-cls`         | Image Classification         |\n",
        "\n",
        "**Available Model Sizes** </br>\n",
        "\n",
        "Each model type comes in multiple sizes for different performance needs:\n",
        "\n",
        "- **n** (Nano) → Smallest, optimized for low-power devices  \n",
        "- **s** (Small) → Balanced for speed and accuracy  \n",
        "- **m** (Medium) → More accurate, moderate speed  \n",
        "- **l** (Large) → High accuracy, requires more computing power  \n",
        "- **x** (Extra Large) → Highest accuracy, most computationally expensive  \n",
        "\n",
        "**Example Usage** </br>\n",
        "\n",
        "To use a specific model:\n",
        "```python\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolo11s-seg.pt\")  # Load the small version of YOLO11 for instance segmentation\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCf2r1vdH6tf"
      },
      "source": [
        "## Download Dataset from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJNC1w3REadY",
        "outputId": "83dc0649-509e-4b94-bc9f-ba7b4080b836"
      },
      "outputs": [],
      "source": [
        "HOME = '/content'\n",
        "!rm -rf {HOME}/datasets\n",
        "!mkdir -p {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "# Update the values for WORKSPACE_ID, PROJECT_ID, API_KEY, and PROJECT_VERSION\n",
        "WORKSPACE_ID = get_token(\"ROBOFLOW_WORKSPACE_ID\")\n",
        "PROJECT_ID = get_token(\"ROBOFLOW_PROJECT_ID\")\n",
        "API_KEY = get_token(\"ROBOFLOW_API_KEY\")\n",
        "PROJECT_VERSION = get_token(\"ROBOFLOW_PROJECT_VERSION\")\n",
        "\n",
        "rf = Roboflow(api_key=API_KEY)\n",
        "project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n",
        "version = project.version(PROJECT_VERSION)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCOMtCvsIPNN"
      },
      "source": [
        "## Training the YOLO Model with Ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JGx7upbg2vc"
      },
      "source": [
        "### General Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7gDbuCUhxW2"
      },
      "source": [
        "\n",
        "**Epochs**\n",
        "\n",
        "The `epochs` parameter defines how many complete passes the model makes through the entire training dataset. Just like in real life, repeating the practice (processing the data) helps the model learn and improve. However, performance gains diminish after a certain point, and too many epochs can lead to overfitting.\n",
        "\n",
        "**Data Augmentation**\n",
        "\n",
        "Data augmentation artificially increases the diversity of your training data, helping the model generalize better. While services like Roboflow can apply augmentations when you prepare your dataset, the Ultralytics API (`model.train()`) also has built-in augmentation capabilities.\n",
        "\n",
        "You can find the full list of augmentations controllable via the Ultralytics training configuration here: [Ultralytics Configuration Docs](https://docs.ultralytics.com/usage/cfg/) (Check the augmentation section).\n",
        "\n",
        "**Our Augmentation Strategy & Cautions:**\n",
        "\n",
        "* **Mosaic:** We will utilize the `mosaic` augmentation (`mosaic=1.0` in parameters). It's a powerful technique that combines multiple images, improving detection of objects in various contexts and scales.\n",
        "* **`fliplr` (Flip Left-Right):** **Be very careful with this!** `fliplr` flips the image horizontally. If your task involves directionality (e.g., detecting *left* vs. *right* lane lines), this augmentation will flip the image *but not the labels*. This means your model will incorrectly learn from images where the visual right lane is labeled as left, and vice-versa. **For such tasks, set `fliplr=0.0` to disable it.**\n",
        "* **Combined Augmentations:** Be mindful if you've already applied augmentations via Roboflow (or another service). Applying heavy augmentations *both* during dataset preparation *and* during Ultralytics training might overlay excessively, potentially making images unrecognizable or confusing for the model. Adjust settings to avoid overly strong combined effects.\n",
        "\n",
        "| **Metric / Loss**  | **Explanation** |\n",
        "|-------------------|---------------|\n",
        "| **Box Loss (`box_loss`)** | Measures how well the model's predicted bounding boxes match the ground truth. Lower is better. |\n",
        "| **Classification Loss (`cls_loss`)** | Evaluates how accurately the model assigns the correct class to detected objects. Lower is better. |\n",
        "| **Distribution Focal Loss (`dfl_loss`)** | Helps refine bounding box locations by improving precision at the edges. Lower is better. |\n",
        "| **Precision (`P`)** | The proportion of predicted objects that are correct (True Positives / (True Positives + False Positives)). Higher means fewer false positives. |\n",
        "| **Recall (`R`)** | The proportion of actual objects that were detected (True Positives / (True Positives + False Negatives)). Higher means fewer false negatives. |\n",
        "| **mAP50 (`Mean Average Precision @ IoU 0.5`)** | Measures how well predicted boxes match ground truth at **Intersection over Union (IoU) ≥ 0.5**. Higher means better accuracy. |\n",
        "| **mAP50-95 (`Mean Average Precision @ IoU 0.5:0.95`)** | A stricter evaluation, averaging mAP over IoU thresholds from **0.5 to 0.95** (harder to score high). Higher means better performance. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZuRRI6_Qb3X"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9zrM4tUTtKE",
        "outputId": "587f65b1-3199-4f56-b051-f54c5be161f1"
      },
      "outputs": [],
      "source": [
        "gpu_count = torch.cuda.device_count()\n",
        "device_str = \",\".join(str(i) for i in range(gpu_count)) if gpu_count > 0 else \"cpu\"\n",
        "\n",
        "print(f\"🧠 Training on: {device_str} ({gpu_count} GPU(s) detected)\")\n",
        "\n",
        "# model = YOLO(\"yolo11n-seg.pt\")\n",
        "model = YOLO(\"yolo11s-seg.pt\") # Load the nano version of YOLO11 for instance segmentation\n",
        "\n",
        "results = model.train(\n",
        "    data=f\"{dataset.location}/data.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=8 * max(gpu_count, 1), # Scale batch size with GPU count\n",
        "    workers=2,\n",
        "    device=device_str, # ✅ Multi-GPU support via comma-separated string\n",
        "    fliplr=0.5,\n",
        "    mosaic=1.0,\n",
        "    degrees=10.0,\n",
        "    translate=0.1,\n",
        "    scale=0.5,\n",
        "    shear=2.0,\n",
        "    perspective=0.001,\n",
        "    project=output_dir + '/runs/segment', # Set project to create the 'runs/segment' subfolders\n",
        "    name='train'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO4yRM4-iONj"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c0Oz3yriPxO"
      },
      "outputs": [],
      "source": [
        "# Load the trained YOLO model from the specified path\n",
        "model_path = base_dir + '/export/runs/segment/train/weights/best.pt'\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Run evaluation on the test split using the provided dataset configuration\n",
        "metrics = model.val(split='test', data=f\"{dataset.location}/data.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VW_qZcFiqzc"
      },
      "source": [
        "### Export as ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Pcme7qitUk"
      },
      "outputs": [],
      "source": [
        "model.export(format=\"onnx\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
